---
image: 20220930-real-offlinerl.png
title: Real World Offline Reinforcement Learning with Realistic Data Sources
excerpt:
  Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevance for real-world robotics. In this work, we posit that data collected from safe operations of closely related tasks are more practical data sources for real-world robot learning. Under these settings, we perform an extensive (800+ hours of data, 270+ human hours evaluations, multiple seeds) empirical study evaluating generalization and transfer capabilities of representative ORL methods on four real-world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning. We release our dataset and implementations at https://sites.google.com/view/real-orl

author: Gaoyue Zhou*, <b>Liyiming Ke*</b>, Siddhartha Srinivasa, Abhinav Gupta, Aravind Rajeswaran, Vikash Kumar
venue: Under submission
year: 2023
arxiv: https://sites.google.com/view/real-orl-anon
pdf: https://sites.google.com/view/real-orl-anon
tags: reinforcement learning
bibtype: inproceedings
bibname: zhou2023orl
bibauthor: Zhou, Gaoyue and Ke, Liyiming and Srinivasa, Siddhartha and Gupta, Abhinav and Rajeswaran, Aravind and Kumar, Vikash
bibbook: .. <!--- International Conference on Robotics and Automation (ICRA) -->
---
