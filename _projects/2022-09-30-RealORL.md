---
image: 20220930-real-offlinerl.png
title: Offline Reinforcement Learning on Real Robot with Realistic Data Sources
excerpt: Offline reinforcement learning (ORL) holds great promise for robot learning due to its ability to learn from arbitrary pre-generated experience. However, current ORL benchmarks are almost entirely in simulation and utilize contrived datasets like replay buffers of online RL agents or sub-optimal trajectories, and thus hold limited relevence for real-world robotics. In this work, we posit that near-optimal and safe demonstrations collected from out-of-domain tasks are more practical data-sources for real world robot learning. We perform an extensive (800+ hours data, 270+ human hours evaluations, multiple seeds) empirical study of representative ORL methods on four real world tabletop manipulation tasks. Our study finds that ORL and imitation learning prefer different action spaces, and that ORL algorithms can generalize from leveraging offline heterogeneous data sources and outperform imitation learning.
author: Gaoyue Zhou*, <b>Liyiming Ke*</b>, Siddhartha Srinivasa, Abhinav Gupta, Aravind Rajeswaran, Vikash Kumar
venue: Under submission
year: 2023
arxiv: https://sites.google.com/view/real-orl-anon
pdf: https://sites.google.com/view/real-orl-anon
tags: reinforcement learning
bibtype: inproceedings
bibname: zhou2023orl
bibauthor: Zhou, Gaoyue and Ke, Liyiming and Srinivasa, Siddhartha and Gupta, Abhinav and Rajeswaran, Aravind and Kumar, Vikash
bibbook: .. <!--- International Conference on Robotics and Automation (ICRA) -->
---
